{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a7257f-e349-4e94-af82-f6349768c24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-12 19:45:40.200050: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFDistilBertForSequenceClassification\n",
    "from transformers import TFDistilBertModel\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from transformers import BertTokenizer, TFDistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "from dataset_vars_load import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef51b701-108b-4aa7-857f-224e9f4fef1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-08-12 19:46:26.555286: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "bert_model = TFDistilBertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "416bac51-787a-4954-a9dc-fad4bb9ebac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Define the classification head\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "num_labels = len(data['label'].unique())  # Assuming unique labels represent number of classes\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=num_labels\n",
    ")\n",
    "\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ad7557-5ccb-47ff-8348-259576e69415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "loss = CategoricalCrossentropy()\n",
    "metrics = ['accuracy']\n",
    "batch_size = 32\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d2adc3a-9d1e-4461-b181-079acd2b3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction using TF-IDF with max 500 features\n",
    "vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 2))\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e6c45bd-d428-43e1-a8a0-ef79810bcb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe53ca-c871-4b99-8439-1b4d53679493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 19:48:09.471817: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (mklcpu) ran out of memory trying to allocate 91.55MiB (rounded to 96000000)requested by op tf_distil_bert_for_sequence_classification/distilbert/transformer/layer_._2/attention/dropout_27/dropout/GreaterEqual\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-08-12 19:48:09.658552: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for mklcpu\n",
      "2024-08-12 19:48:09.671720: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-12 19:48:09.672976: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-12 19:48:09.673000: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-12 19:48:09.673015: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-12 19:48:09.673028: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-12 19:48:09.673041: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-12 19:48:09.673055: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-12 19:48:09.673068: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-12 19:48:09.673081: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-12 19:48:09.673094: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-12 19:48:09.673107: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-12 19:48:09.694285: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 1, Chunks in use: 1. 625.0KiB allocated for chunks. 625.0KiB in use in bin. 625.0KiB client-requested in use in bin.\n",
      "2024-08-12 19:48:09.700872: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 4, Chunks in use: 2. 6.10MiB allocated for chunks. 3.00MiB in use in bin. 3.00MiB client-requested in use in bin.\n",
      "2024-08-12 19:48:09.701640: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 101, Chunks in use: 101. 233.42MiB allocated for chunks. 233.42MiB in use in bin. 225.75MiB client-requested in use in bin.\n",
      "2024-08-12 19:48:09.701669: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-12 19:48:09.701688: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 55, Chunks in use: 55. 522.53MiB allocated for chunks. 522.53MiB in use in bin. 514.03MiB client-requested in use in bin.\n",
      "2024-08-12 19:48:09.701703: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-08-12 19:48:09.701720: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 3, Chunks in use: 3. 140.62MiB allocated for chunks. 140.62MiB in use in bin. 140.62MiB client-requested in use in bin.\n",
      "2024-08-12 19:48:09.701737: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 7, Chunks in use: 6. 602.26MiB allocated for chunks. 535.09MiB in use in bin. 498.24MiB client-requested in use in bin.\n",
      "2024-08-12 19:48:09.701772: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 1. 176.24MiB allocated for chunks. 176.24MiB in use in bin. 89.42MiB client-requested in use in bin.\n",
      "2024-08-12 19:48:09.701790: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 1. 366.21MiB allocated for chunks. 366.21MiB in use in bin. 366.21MiB client-requested in use in bin.\n",
      "2024-08-12 19:48:09.703137: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 91.55MiB was 64.00MiB, Chunk State: \n",
      "2024-08-12 19:48:09.943793: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 67.17MiB | Requested Size: 0B | in_use: 0 | bin_num: 18, prev:   Size: 46.88MiB | Requested Size: 46.88MiB | in_use: 1 | bin_num: -1\n",
      "2024-08-12 19:48:10.483211: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 1073741824\n",
      "2024-08-12 19:48:11.115794: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7659cac040 of size 1572864 next 119\n",
      "2024-08-12 19:48:11.571250: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f7659e2c040 of size 2359296 next 120\n",
      "2024-08-12 19:48:11.776272: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f765a06c040 of size 2359296 next 121\n",
      "2024-08-12 19:48:12.260884: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f765a2ac040 of size 2359296 next 122\n",
      "2024-08-12 19:48:12.694941: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f765a4ec040 of size 2359296 next 123\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_features.toarray().astype(int), tf.keras.utils.to_categorical(y_train), batch_size=batch_size, epochs=epochs, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
