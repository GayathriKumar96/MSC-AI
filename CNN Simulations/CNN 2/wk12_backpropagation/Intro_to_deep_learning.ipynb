{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68dd435e",
   "metadata": {},
   "source": [
    "# Introduction to deep learning\n",
    "Lisa Bonheme and Marek Grzes\n",
    "\n",
    "University of Kent\n",
    "\n",
    "COMP6360/8360, Teaching week 12\n",
    "\n",
    "Last modified 13/11/2022\n",
    "\n",
    "## Content\n",
    "- [Installing Tensorflow on Anaconda](#Installing-Tensorflow-on-Anaconda)\n",
    "- [Overview of the Moon dataset](#Overview-of-the-Moon-dataset)\n",
    "- [Question 1 - Comparing logistic regression and deep learning](#Question-1---Comparing-logistic-regression-and-deep-learning)\n",
    "- [Question 2 - The decision boundaries of deep neural networks](#Question-2---The-decision-boundaries-of-deep-neural-networks)\n",
    "- [Question 3 - Implement the backpropagation algorithm](#Question-3---Implement-the-backpropagation-algorithm)\n",
    "- [Question 4 - Tune your model](#Question-4---Tune-your-model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead683c",
   "metadata": {},
   "source": [
    "### Installing Tensorflow on Anaconda\n",
    "During this class, we will need tensorflow 2, which is not installed in miniconda by default.\n",
    "You can install it using the anaconda navigator as follows:\n",
    "- **Step 1**: Open anaconda navigator.<br/>\n",
    "<img src=\"img/anaconda-step1.jpg\"></img><br/>\n",
    "\n",
    "- **Step 2**: Click on the environments tab and search Tensorflow 2 package.<br/>\n",
    "<img src=\"img/anaconda-step2.jpg\"></img><br/>\n",
    "\n",
    "- **Step 3**: Select Tensorflow 2 package and install it.<br/>\n",
    "<img src=\"img/anaconda-step3.jpg\"></img><br/>\n",
    "\n",
    "(Images are taken from [this tutorial](https://www.tutorialspoint.com/add-packages-to-anaconda-environment-in-python))\n",
    "\n",
    "You can also install `tensorflow` and other Python packages using `pip`. For that, you need to open a terminal window, and assuming that your Python environment is active in that window, you can install `tensorflow` typing: `pip install tensorflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695668e3",
   "metadata": {},
   "source": [
    "### Overview of the Moon dataset\n",
    "The Moon dataset is an artificial dataset with two intertwined moon shapes belonging to two different classes.\n",
    "\n",
    "The dataset is composed of 10000 data points with the following features:\n",
    "- x position\n",
    "- y position\n",
    "\n",
    "So, our data matrix is of size (10000, 2); that is (`nb_data_points`, `nb_features`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224b7cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:19.819639Z",
     "iopub.status.busy": "2022-11-14T21:24:19.817360Z",
     "iopub.status.idle": "2022-11-14T21:24:20.536308Z",
     "shell.execute_reply": "2022-11-14T21:24:20.535902Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "from sklearn import datasets\n",
    "\n",
    "# Here we add some noise so that some points are a bit outside of the moon shapes\n",
    "X, y = datasets.make_moons(n_samples=10000, noise=0.05)\n",
    "\n",
    "print(\"The data shape is {}\\n\".format(X.shape))\n",
    "print(\"The first 5 data points are \\n{}\\n\".format(X[:5]))\n",
    "print(\"The first 5 labels are {}\\n\".format(y[:5]))\n",
    "print(\"The last 5 data points are \\n{}\\n\".format(X[-5:]))\n",
    "print(\"The last 5 labels are {}\".format(y[-5:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5e0eb",
   "metadata": {},
   "source": [
    "Now, let us visualise what this dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc8ee04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:20.539348Z",
     "iopub.status.busy": "2022-11-14T21:24:20.538799Z",
     "iopub.status.idle": "2022-11-14T21:24:21.084145Z",
     "shell.execute_reply": "2022-11-14T21:24:21.084487Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "ax = sns.scatterplot(x=X[:,0], y=X[:,1], hue=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d73b2d",
   "metadata": {},
   "source": [
    "### Question 1 - Comparing logistic regression and deep learning\n",
    "Now that we have our dataset, we will create a function to plot the decision boundary of our models and use it to compare the behaviour of a logistic regression and a deep learning model. Note that logistic regression is another name for our familiar delta rule when the sigmoid activation function is used.\n",
    "\n",
    "#### Plotting decision boundaries\n",
    "Nothing to do here, but you can explore this function to see what it does if you are curious about it.\n",
    "\n",
    "Note that this part of the class is inspired by [scikit learn example on multinomial logistic regression](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html#sphx-glr-auto-examples-linear-model-plot-logistic-multinomial-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b943f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:21.089611Z",
     "iopub.status.busy": "2022-11-14T21:24:21.089258Z",
     "iopub.status.idle": "2022-11-14T21:24:21.091010Z",
     "shell.execute_reply": "2022-11-14T21:24:21.090720Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function will plot the decision boundaries of any model given our 2D dataset. We will use this function many times in this class.\n",
    "def plot_decision_boundaries(X, y, clf, step=None):\n",
    "     # create a mesh to plot in\n",
    "    h = 0.02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Pastel1)\n",
    "    plt.axis(\"tight\")\n",
    "    \n",
    "    # If we are plotting the decision boundaries sequentially, we mention the corresponding epoch\n",
    "    if step is not None:\n",
    "        plt.title(\"Decision boundaries at epoch {}\".format(step))\n",
    "\n",
    "    # Plot also the training points\n",
    "    colors = [\"blue\", \"orange\", \"green\"]\n",
    "    for i, color in zip(range(3), colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Pastel1, edgecolor=\"black\", s=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a1b3da",
   "metadata": {},
   "source": [
    "#### The decision boundary of a logistic regression model\n",
    "Let us train a logistic regression model and plot its decision boundaries after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e337a80c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:21.093513Z",
     "iopub.status.busy": "2022-11-14T21:24:21.093180Z",
     "iopub.status.idle": "2022-11-14T21:24:21.360996Z",
     "shell.execute_reply": "2022-11-14T21:24:21.360642Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# delta rule is used in the next line of code to fit the model to the data\n",
    "clf = LogisticRegression(max_iter=100, random_state=0).fit(X, y)\n",
    "print(\"The decision boundary of the multinomial logistic regression\")\n",
    "plot_decision_boundaries(X, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69bbee",
   "metadata": {},
   "source": [
    "- **(1-1):** Is the model accurately classifying every data point? Justify your answer.\n",
    "\n",
    "#### The decision boundary of a deep neural network\n",
    "Now, let us create a simple neural network with no hidden layers and train it for a few epochs.\n",
    "\n",
    "At the end of each epoch, we will plot the decision boundaries using the `BoundariesCallback` defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b054b17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:21.364273Z",
     "iopub.status.busy": "2022-11-14T21:24:21.363937Z",
     "iopub.status.idle": "2022-11-14T21:24:22.625092Z",
     "shell.execute_reply": "2022-11-14T21:24:22.624673Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "class BoundariesCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, X, y, clf, plot_freq=2):\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        self._clf = clf\n",
    "        self._plot_freq = plot_freq\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self._plot_freq == 0:\n",
    "            plot_decision_boundaries(self._X, self._y, self._clf, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446104e4",
   "metadata": {},
   "source": [
    "Below is the deep model that we will use in the rest of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba4b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:22.630525Z",
     "iopub.status.busy": "2022-11-14T21:24:22.630147Z",
     "iopub.status.idle": "2022-11-14T21:24:22.632032Z",
     "shell.execute_reply": "2022-11-14T21:24:22.631666Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class DeepModel:\n",
    "    def __init__(self, n_units=[], activation_functions=[], learning_rate=0.005):\n",
    "        self._history = None\n",
    "\n",
    "        # This sequential model can be used to sequentially add layers.\n",
    "        self._model = keras.Sequential()\n",
    "        input_dim=2\n",
    "        \n",
    "        # We set the linear layers according to the given parameters\n",
    "        for units, act in zip(n_units, activation_functions):\n",
    "            self._model.add(keras.layers.Dense(units=units, activation=act, input_dim=input_dim))\n",
    "            if input_dim is not None:\n",
    "                input_dim = None\n",
    "                \n",
    "        self._model.add(keras.layers.Dense(units=1, input_dim=input_dim, activation='sigmoid'))\n",
    "        \n",
    "        \n",
    "        # This will output the final architecture of the model\n",
    "        self._model.summary()\n",
    "        \n",
    "        # We compile the model with a specific loss and optimiser, it is here that the learning rate is set\n",
    "        self._model.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.round(self._model.predict(X))\n",
    "    \n",
    "    def fit(self, X, y, callbacks=[], epochs=11, validation_split=0.2):\n",
    "        # If you don't want to see the training log, you can add verbose=0 to the fit method's arguments below.\n",
    "        self._history = self._model.fit(X, y, validation_split=validation_split, epochs=epochs, \n",
    "                                        batch_size=50, callbacks=callbacks)\n",
    "        return self._history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea2a251",
   "metadata": {},
   "source": [
    "Now, we have the definition of our deep learning model, and we can compute its decision boundaries during training.\n",
    "\n",
    "Below, we are going to create and train a very simple neural network with no hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360b004",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:22.634819Z",
     "iopub.status.busy": "2022-11-14T21:24:22.634451Z",
     "iopub.status.idle": "2022-11-14T21:24:29.158088Z",
     "shell.execute_reply": "2022-11-14T21:24:29.158625Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "no_hidden_layer = DeepModel()\n",
    "\n",
    "# You can change how often the decision boundaries are displayed by modifying plot_freq below. \n",
    "# Here we display the boundaries every 2 epochs, set it to 1 to display after every epochs for example.\n",
    "callbacks = [BoundariesCallback(X, y, no_hidden_layer, plot_freq=2)]\n",
    "\n",
    "# If you want to see how the model evolves after a more training time, you can increase the epochs parameter below\n",
    "res = no_hidden_layer.fit(X, y, callbacks=callbacks, epochs=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70722e5d",
   "metadata": {},
   "source": [
    "- **(1-2)** Compare the decision boundaries of this neural network with the decision boundaries of the logistic regression model computed earlier? Explain and justify your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a6af2",
   "metadata": {},
   "source": [
    "### Question 2 - The decision boundaries of deep neural networks\n",
    "Let us repeat the last experiment with a hidden layer added to our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5688a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:29.164156Z",
     "iopub.status.busy": "2022-11-14T21:24:29.163624Z",
     "iopub.status.idle": "2022-11-14T21:24:37.016234Z",
     "shell.execute_reply": "2022-11-14T21:24:37.016565Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "# We add one hidden layer with 50 neurons and a linear activation.\n",
    "# The linear activation is used by default.\n",
    "one_hidden_layer = DeepModel(n_units=[50], activation_functions=[None])\n",
    "\n",
    "# You can change how often the decision boundaries are displayed by modifying plot_freq below. \n",
    "# Here we display the boundaries every 2 epochs, set it to 1 to display after every epochs for example.\n",
    "callbacks = [BoundariesCallback(X, y, one_hidden_layer, plot_freq=2)]\n",
    "\n",
    "# If you want to see how the model evolves after a more training time, you can increase the epochs parameter below\n",
    "res = one_hidden_layer.fit(X, y, callbacks=callbacks, epochs=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd717d9",
   "metadata": {},
   "source": [
    "- **(2-1)** Is the new model with a hidden layer better than the previous one, i.e., is the new decision boundary more accurate?\n",
    "- **(2-2)** How do the training stages of both models differ? What similarities / differences can you identify?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c50431",
   "metadata": {},
   "source": [
    "#### Impact of the activation function\n",
    "Using a deep neural network with one hidden layer, let us now investigate the impact of activation functions on our hidden layer and the final predictions.\n",
    "\n",
    "Here, we use the \"ReLU\" function, which stands for Rectified Linear Unit. This function transforms the outputs of your hidden layer by keeping only the positive part, so that $f(x) = \\max(0, x)$. In this formula, $x$ is the net input and $f(x)$ is the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da18e18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:37.021657Z",
     "iopub.status.busy": "2022-11-14T21:24:37.021294Z",
     "iopub.status.idle": "2022-11-14T21:24:44.586172Z",
     "shell.execute_reply": "2022-11-14T21:24:44.585587Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "# We add one hidden layer with 50 neurons and relu activation function\n",
    "one_hidden_layer_relu = DeepModel(n_units=[50], activation_functions=[\"relu\"])\n",
    "\n",
    "# You can change how often the decision boundaries are displayed by modifying plot_freq below. \n",
    "# Here we display the boundaries every 2 epochs, set it to 1 to display after every epochs for example.\n",
    "callbacks = [BoundariesCallback(X, y, one_hidden_layer_relu, plot_freq=2)]\n",
    "\n",
    "# If you wish to see how the model evolves after more training time,\n",
    "# you can increase the epochs parameter below.\n",
    "res = one_hidden_layer_relu.fit(X, y, callbacks=callbacks, epochs=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c586151",
   "metadata": {},
   "source": [
    "- **(2-3)** How different are the decision boundaries learned with the new activation function? Explain and justify those differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8fdc9",
   "metadata": {},
   "source": [
    "#### Impact of the learning rate\n",
    "The default learning rate is 0.005. Change its value and observe and then analyse the results. You are encouraged to run this test with both linear and ReLU activations in the hidden layer. See the comments in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a518b030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:44.592021Z",
     "iopub.status.busy": "2022-11-14T21:24:44.591619Z",
     "iopub.status.idle": "2022-11-14T21:24:52.105314Z",
     "shell.execute_reply": "2022-11-14T21:24:52.105844Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "# Choose your learning rate here\n",
    "lr = 0.005\n",
    "\n",
    "one_hidden_layer_lr = DeepModel(learning_rate=lr)\n",
    "callbacks = [BoundariesCallback(X, y, one_hidden_layer_lr, plot_freq=2)]\n",
    "# This network has linear units in the hidden layer.\n",
    "res = one_hidden_layer_lr.fit(X, y, callbacks=callbacks, epochs=11)\n",
    "# Use the line below to test the network with ReLU in the hidden layer. Both networks were defined above.\n",
    "# res = one_hidden_layer_relu.fit(X, y, callbacks=callbacks, epochs=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b583cd0",
   "metadata": {},
   "source": [
    "- **(2-4)** What happens with a very small learning rate?\n",
    "- **(2-5)** What happens with a high learning rate?\n",
    "- **(2-6)** Explain and justify the observations that you made in the last two steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6a0b8",
   "metadata": {},
   "source": [
    "### Question 3 - Implement the backpropagation algorithm (from scratch)\n",
    "Now that you have seen how a deep model could be implemented using tensorflow 2, you will create your own implementation of a deep learning algorithm! We ask you to code the backpropagation algorithm that was presented in our lectures. All the equations required for this implementation are in our lecture slides. You will need to transfer them to your Python code below.\n",
    "\n",
    "#### The sigmoid function\n",
    "We have previously used the sigmoid or ReLU activation function in the last layer of our deep model, and we will need it again for this question. If we use sigmod in this section, we can reuse the equations that we have in our lecture slides.\n",
    "\n",
    "- **(3-1)** Implement the sigmoid function below using the formula seen in the lectures. You can see it also [here](https://en.wikipedia.org/wiki/Sigmoid_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9039803",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:52.108906Z",
     "iopub.status.busy": "2022-11-14T21:24:52.108457Z",
     "iopub.status.idle": "2022-11-14T21:24:52.110612Z",
     "shell.execute_reply": "2022-11-14T21:24:52.110046Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # TODO: Comment the line below and implement me!\n",
    "    raise NotImplementedError(\"Implement the sigmoid function before testing.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85fde8",
   "metadata": {},
   "source": [
    "#### The backpropagation algorithm\n",
    "Now that our sigmoid function is ready, let us define a skeleton for our custom deep learning algorithm.\n",
    "\n",
    "The backpropagation algorithm for a feedforward network of two layers of sigmoid units can be defined as follows:\n",
    "\n",
    "**For each** $(x, y)$ in the training examples, **DO**:\n",
    "<ul>\n",
    "    <li>\n",
    "        <i>Propagate the input forward through the network:</i>\n",
    "        <ul>\n",
    "            <li> \n",
    "                1. Input the instance $x$ to the network and compute the output $o_u$ of every unit $u$ in the network.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        <i>Propagate the errors backward through the network:</i>\n",
    "        <ul>\n",
    "            <li>\n",
    "                2. For each network output unit $k$, calculate its error term\n",
    "                $\\delta_k \\leftarrow -(y_k - o_k)$</br>\n",
    "                (here, $y_k$ is our $t_k$, and we don't multiply by $o_k(1-o_k)$ because we assume the CE error)\n",
    "                </li>\n",
    "            <li>\n",
    "                3. For each hidden unit $h$, calculate its error term\n",
    "                $\\delta_h = - o_h(1- o_h) \\sum_{k \\in outputs} w_{hk}\\delta_k$\n",
    "            </li>\n",
    "         </ul>    \n",
    "    </li>\n",
    "    <li>\n",
    "        <i>Update the network weights:</i>\n",
    "        <ul>\n",
    "            <li>\n",
    "                4. Update each network weight $w_{ji} = w_{ji} + \\Delta w_{ji}$</br>where\n",
    "                $\\Delta w_{ji} = -\\epsilon\\delta_jx_{j}$ or $\\Delta w_{ji} = -\\epsilon\\delta_jh_{j}$ and $\\epsilon$ is the learning rate.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "This procedure is based on Table 4.2 in _Machine learning, Tom Mitchell, McGraw-Hill Education, 1997_. Note that the example in this book assumes that the output unit uses sigmoid activation and the SSE error is optimised. In our discussion of the delta rule with sigmoid activation, we assumed the CE error, which made the update equation of the output units slightly simpler. We used this assumption in the pseudocode above.\n",
    "\n",
    "Using the class `MyDeepModel` below, do the following:\n",
    "- **(3-2)** Implement the `forward()` method which corresponds to step 1 of the backpropagation algorithm.\n",
    "- **(3-3)** Implement the `backward()` method which corresponds to steps 2 and 3 of the backpropagation algorithm.\n",
    "- **(3-4)** Implement the `update()` method which corresponds to step 4 of the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b312c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:52.113713Z",
     "iopub.status.busy": "2022-11-14T21:24:52.113374Z",
     "iopub.status.idle": "2022-11-14T21:24:52.115087Z",
     "shell.execute_reply": "2022-11-14T21:24:52.114750Z"
    }
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\" This is a convenience class that we will use to define layers in our deep learning model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, units, random_state):\n",
    "        self._random_state = random_state\n",
    "        # we initialise the weights randomly\n",
    "        self.weights = self._random_state.normal(size=(input_dim, units))\n",
    "        # we add the weight separately to retrieve it more easily afterwards.\n",
    "        self.bias = self._random_state.normal(size=units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd3344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:52.123050Z",
     "iopub.status.busy": "2022-11-14T21:24:52.122674Z",
     "iopub.status.idle": "2022-11-14T21:24:52.124343Z",
     "shell.execute_reply": "2022-11-14T21:24:52.124000Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class MyDeepModel:\n",
    "    # learning rate seems to be a sensitive parameter\n",
    "    def __init__(self, learning_rate=0.02):\n",
    "        self._random_state = np.random.RandomState(0)\n",
    "        self._history = []\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.layers = [\n",
    "            # This is the layer corresponding to something like \n",
    "            # self._model.add(keras.layers.Dense(10, input_dim=2, activatoin=\"sigmoid\"))\n",
    "            # in our DeepModel\n",
    "            Layer(2, 10, self._random_state),\n",
    "            \n",
    "            # This is the output layer corresponding to \n",
    "            # self._model.add(keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "            # in our DeepModel. Note that now we pass the object sigmoid as parameter, not the string.\n",
    "            Layer(10, 1, self._random_state)\n",
    "        ]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" Takes input data of shape (n_items, n_features) and returns an array of shape (n_items,) \n",
    "        containing the labels predicted for each input x\n",
    "        \"\"\"\n",
    "        y_pred = []\n",
    "        \n",
    "        # Here we register the predictions of the model for each input\n",
    "        # and store the results in a list\n",
    "        for x in X:\n",
    "            y_pred.append(self.forward(x)[-1])\n",
    "        y_pred = np.round(np.array(y_pred))\n",
    "        return y_pred\n",
    "    \n",
    "    def fit(self, X, y, epochs=11, validation_split=0.2):\n",
    "        \"\"\" Train the model during a given number of epochs and plot the decision boundaries after each epoch.\n",
    "        This method performs one step of the backpropagation algorithm for each data example\n",
    "        by calling the forward, backward and update methods.\n",
    "        \"\"\"\n",
    "        self._history = {\"train\": [], \"test\": []}\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=validation_split, \n",
    "                                                            random_state=self._random_state)\n",
    "        \n",
    "        # One epoch corresponds to the training steps needed to train the model on the whole dataset once\n",
    "        for i in range(epochs):\n",
    "            for xt, yt in zip(X_train, y_train):\n",
    "                # The forward step predicts the class of the input\n",
    "                try:\n",
    "                    outputs = self.forward(xt)\n",
    "                    # The backward step backpropagate the error\n",
    "                    gradients = self.backward(outputs, yt)\n",
    "                    # Now, we update the weights\n",
    "                    self.update(xt, outputs, gradients)\n",
    "                except NotImplementedError as e:\n",
    "                    print(e)\n",
    "                    return\n",
    "            \n",
    "            y_pred = self.predict(X_train)\n",
    "            acc_train = accuracy_score(y_train, y_pred)   \n",
    "            self._history[\"train\"].append(acc_train)\n",
    "            y_pred = self.predict(X_test)\n",
    "            acc_test = accuracy_score(y_test, y_pred)\n",
    "            self._history[\"test\"].append(acc_test)\n",
    "            print(\"Accuracy at epoch {}:\\ntrain: {}\\ntest: {}\".format(i, acc_train, acc_test))\n",
    "            plot_decision_boundaries(X, y, self, step=i)\n",
    "        return self._history\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass of the backpropagation \n",
    "        Takes the one data example as input and returns the output values of each layer\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        # TODO: Comment the line below and implement me!\n",
    "        raise NotImplementedError(\"Implement the forward function before testing.\")\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def backward(self, outputs, y_true):\n",
    "        \"\"\" Backward pass of the backpropagation algorithm\n",
    "        Takes the output of the layer obtained in forward pass, compute and backpropagate the error,\n",
    "        and returns the obtained gradients\n",
    "        \"\"\"\n",
    "        gradients = []\n",
    "        # TODO: Comment the line below and implement me!\n",
    "        raise NotImplementedError(\"Implement the backward function before testing.\")\n",
    "\n",
    "        return gradients\n",
    "    \n",
    "    def update(self, x, outputs, gradients):\n",
    "        \"\"\" Update the weights after the backward pass of the backpropagation algorithm\n",
    "        Takes the gradient obtained during the backpropagation and update the weights of each layer\n",
    "        \"\"\"\n",
    "        # TODO: Comment the line below and implement me!\n",
    "        raise NotImplementedError(\"Implement the update function before testing.\")\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b4e6d7",
   "metadata": {},
   "source": [
    "- **(3-5)** Test your implementation and compare its results with the Keras implementation using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea3df0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:52.127167Z",
     "iopub.status.busy": "2022-11-14T21:24:52.126760Z",
     "iopub.status.idle": "2022-11-14T21:24:52.129784Z",
     "shell.execute_reply": "2022-11-14T21:24:52.129323Z"
    }
   },
   "outputs": [],
   "source": [
    "my_model = MyDeepModel()\n",
    "print(\"Training my model\")\n",
    "res = my_model.fit(X, y, epochs=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0987c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:24:52.133665Z",
     "iopub.status.busy": "2022-11-14T21:24:52.133077Z",
     "iopub.status.idle": "2022-11-14T21:25:02.950955Z",
     "shell.execute_reply": "2022-11-14T21:25:02.950599Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_model = DeepModel(n_units=[10], activation_functions=[\"sigmoid\"])\n",
    "print(\"Training Tensorflow model\")\n",
    "callbacks = [BoundariesCallback(X, y, tf_model, plot_freq=1)]\n",
    "res = tf_model.fit(X, y, callbacks=callbacks, epochs=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb1e53",
   "metadata": {},
   "source": [
    "### Question 4 - Tune your model\n",
    "If you have finished early, and you'd like something more challenging to do, you can add a few options to refine your model. For example:\n",
    "- backpropagate through multiple hidden layers,\n",
    "- handle different batch sizes,\n",
    "- do backpropagation with other activation functions,\n",
    "- implement a stopping condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca7f88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-14T21:25:02.953715Z",
     "iopub.status.busy": "2022-11-14T21:25:02.953380Z",
     "iopub.status.idle": "2022-11-14T21:25:02.955907Z",
     "shell.execute_reply": "2022-11-14T21:25:02.955539Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(\"Last modified: \", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
