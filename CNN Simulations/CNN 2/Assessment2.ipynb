{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c73943-6310-41f8-91fd-2707e05a4701",
   "metadata": {},
   "source": [
    "**(2-4):** Run your implementation of the Hebb rule on `data_HL_simple.txt`, and inspect the weights $w$ after every step of the algorithm. Identify the correlational pattern that the algorithm discovers. Are there any positively or negatively correlated features in these data according to the pattern seen in $w$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "458d8df2-f91f-4835-87aa-bab1e3d54061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  0. -1. -1.  1.]\n",
      " [-1. -1. -1. -1.  1.]\n",
      " [ 1.  0.  1.  1. -1.]\n",
      " [ 0. -1.  0.  0.  0.]]\n",
      "initial weights\n",
      "[-0.75274198  0.47185075  0.17669483 -0.17238758 -0.74879497]\n",
      "\tCurrent input: [0. 0. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.74666373  0.49456929  0.21123663 -0.13784577 -0.78333677]\n",
      "\tCurrent input: [-1.  0. -1. -1.  1.]\n",
      "New Weights:\n",
      "[-0.75577297  0.5089104   0.2459072  -0.1031752  -0.81800734]\n",
      "\tCurrent input: [0. 1. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.76841783  0.51397529  0.28610095 -0.06298145 -0.85820109]\n",
      "\tCurrent input: [ 0.  0.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.81847108  0.54832845  0.38246186  0.03337946 -0.954562  ]\n",
      "\tCurrent input: [ 1.  1.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.8127617   0.55875029  0.46476616  0.11568375 -1.0368663 ]\n",
      "\tCurrent input: [0. 0. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.77049278  0.60319549  0.56538831  0.2163059  -1.13748844]\n",
      "\tCurrent input: [-1.  0. -1. -1.  1.]\n",
      "New Weights:\n",
      "[-0.75498895  0.63381081  0.65707259  0.30799018 -1.22917273]\n",
      "\tCurrent input: [0. 1. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.76507437  0.64397627  0.74887122  0.39978881 -1.32097136]\n",
      "\tCurrent input: [ 0.  0.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.83368515  0.69668403  0.95297412  0.60389171 -1.52507426]\n",
      "\tCurrent input: [ 1.  1.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.78498411  0.69839615  1.16403363  0.81495122 -1.73613376]\n",
      "\tCurrent input: [0. 0. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.64827031  0.79546473  1.43134772  1.08226532 -2.00344786]\n",
      "\tCurrent input: [-1.  0. -1. -1.  1.]\n",
      "New Weights:\n",
      "[-0.56482475  0.86668668  1.66552544  1.31644304 -2.23762558]\n",
      "\tCurrent input: [0. 1. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.56440138  0.89026994  1.88389379  1.53481138 -2.45599392]\n",
      "\tCurrent input: [ 0.  0.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.66997761  0.98391191  2.3495059   2.00042349 -2.92160603]\n",
      "\tCurrent input: [ 1.  1.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.50860316  0.96186078  2.88055218  2.53146977 -3.45265232]\n",
      "\tCurrent input: [0. 0. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.13206907  1.18815762  3.56473301  3.21565061 -4.13683315]\n",
      "\tCurrent input: [-1.  0. -1. -1.  1.]\n",
      "New Weights:\n",
      "[ 0.12813795  1.36064697  4.15370109  3.80461869 -4.72580123]\n",
      "\tCurrent input: [0. 1. 0. 0. 0.]\n",
      "New Weights:\n",
      "[ 0.15965596  1.41868849  4.6843574   4.335275   -5.25645754]\n",
      "\tCurrent input: [ 0.  0.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.02703388  1.6086842   5.79201293  5.44293052 -6.36411307]\n",
      "\tCurrent input: [ 1.  1.  1.  1. -1.]\n",
      "New Weights:\n",
      "[ 0.42100848  1.5255826   7.1174877   6.76840529 -7.68958783]\n",
      "\tCurrent input: [0. 0. 0. 0. 0.]\n",
      "New Weights:\n",
      "[ 1.39906803  2.07115408  8.83992858  8.49084617 -9.41202871]\n",
      "\tCurrent input: [-1.  0. -1. -1.  1.]\n",
      "New Weights:\n",
      "[  2.10762323   2.4958199   10.31069464   9.96161223 -10.88279478]\n",
      "\tCurrent input: [0. 1. 0. 0. 0.]\n",
      "New Weights:\n",
      "[  2.22236472   2.64106422  11.61410788  11.26502548 -12.18620802]\n",
      "\tCurrent input: [ 0.  0.  1.  1. -1.]\n",
      "New Weights:\n",
      "[  1.84681128   3.06425139  14.30683394  13.95775154 -14.87893408]\n",
      "\tCurrent input: [ 1.  1.  1.  1. -1.]\n",
      "New Weights:\n",
      "[  3.01455766   2.82775391  17.60386228  17.25477988 -18.17596242]\n",
      "\tCurrent input: [0. 0. 0. 0. 0.]\n",
      "New Weights:\n",
      "[  5.492813     4.16416709  21.90719042  21.55810801 -22.47929056]\n",
      "\tCurrent input: [-1.  0. -1. -1.  1.]\n",
      "New Weights:\n",
      "[  7.32549678   5.21599027  25.56770026  25.21861785 -26.1398004 ]\n",
      "\tCurrent input: [0. 1. 0. 0. 0.]\n",
      "New Weights:\n",
      "[  7.65408902   5.58002188  28.78600212  28.43691972 -29.35810226]\n",
      "\tCurrent input: [ 0.  0.  1.  1. -1.]\n",
      "New Weights:\n",
      "[  6.82442363   6.57518952  35.40231829  35.05323588 -35.97441842]\n",
      "\tCurrent input: [ 1.  1.  1.  1. -1.]\n",
      "New Weights:\n",
      "[  9.78792252   5.9564876   43.59152925  43.24244684 -44.16362938]\n",
      "[  9.78792252   5.9564876   43.59152925  43.24244684 -44.16362938]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load data from the text file\n",
    "data = np.loadtxt('data_HL_simple.txt', usecols=range(5))\n",
    "print(data[1:5,:])\n",
    "\n",
    "# we need a vector of 5 parameters\n",
    "w = np.random.normal(0, 0.5, 5)\n",
    "\n",
    "print(\"initial weights\")\n",
    "print(w)\n",
    "\n",
    "# learning rate; don't change the learning rate epsilon when you are working on questions 2 and 3, unless you really know what you are doing. You may want to make the learning rate dependent on the epoch number to converge to a stable solution.\n",
    "epsilon = 0.01\n",
    "\n",
    "num_iterations = 6\n",
    "\n",
    "update = 0\n",
    "# we can do a fixed number of iterations because the simple Hebb rule won't converge\n",
    "for i in range(num_iterations):\n",
    "    for x in data:\n",
    "\n",
    "        # TODO: add your code here; note that \"epsilon\"\n",
    "        # is defined above and it should be used here;\n",
    "        # don't change its default value in this question.\n",
    "\n",
    "        y = np.dot(x,w)\n",
    "        delta_w = epsilon*y*x\n",
    "\n",
    "        w = w+delta_w\n",
    "        \n",
    "        update = update + 1\n",
    "        if update % 10 == 0:\n",
    "            # print every 10th update\n",
    "            print(\"\\tCurrent input: \" + str(x))\n",
    "            # print the weights for every x to see how they are evolving\n",
    "            print(\"New Weights:\\n\" + str(w))\n",
    "\n",
    "# the final weights\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70763b-ac5f-4239-9880-2f27f6ffbe5d",
   "metadata": {},
   "source": [
    "**(3-4):** The next step requires coding. Extend the code that implements simple Hebbian learning above and implement Oja's rule to learn one principal component for these data. We are interested in exactly one principal component in this exercise. Your implementation of Oja's rule should appear in the Python block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d62c39c7-911b-4258-b835-ff995b56469a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  0. -1. -1.  1.]\n",
      " [-1. -1. -1. -1.  1.]\n",
      " [ 1.  0.  1.  1. -1.]\n",
      " [ 0. -1.  0.  0.  0.]]\n",
      "initial weights\n",
      "[-0.37397226 -0.8479258   0.27563584 -0.30436486  0.78693894]\n",
      "\tCurrent input: [0. 0. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.37810345 -0.77866597  0.15133444 -0.35119272  0.76930702]\n",
      "\tCurrent input: [-1.  0. -1. -1.  1.]\n",
      "New Weights:\n",
      "[-0.38608321 -0.73456464  0.07227123 -0.37184462  0.74135935]\n",
      "\tCurrent input: [0. 1. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.38094074 -0.7127951   0.02602661 -0.38358669  0.72439449]\n",
      "\tCurrent input: [ 0.  0.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.3349736  -0.66318382 -0.0564315  -0.42224089  0.72660281]\n",
      "\tCurrent input: [ 1.  1.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.33447137 -0.60342161 -0.12997693 -0.4468358   0.71046976]\n",
      "\tCurrent input: [0. 0. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.33623349 -0.53347734 -0.21281614 -0.46987029  0.68374532]\n",
      "\tCurrent input: [-1.  0. -1. -1.  1.]\n",
      "New Weights:\n",
      "[-0.34045132 -0.49749214 -0.25814361 -0.47860606  0.66203595]\n",
      "\tCurrent input: [0. 1. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.32696464 -0.47541601 -0.28790134 -0.48756518  0.65369014]\n",
      "\tCurrent input: [ 0.  0.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.26947984 -0.42670159 -0.34211813 -0.51174436  0.65287733]\n",
      "\tCurrent input: [ 1.  1.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.26938149 -0.36743594 -0.38378336 -0.52491749  0.64234437]\n",
      "\tCurrent input: [0. 0. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.28073895 -0.33076988 -0.41736243 -0.52861372  0.62117739]\n",
      "\tCurrent input: [-1.  0. -1. -1.  1.]\n",
      "New Weights:\n",
      "[-0.28736044 -0.31383096 -0.43527883 -0.52962137  0.60811657]\n",
      "\tCurrent input: [0. 1. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.27094787 -0.29790165 -0.45037059 -0.53486901  0.60517367]\n",
      "\tCurrent input: [ 0.  0.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.21444812 -0.26650031 -0.47792506 -0.54795309  0.60621805]\n",
      "\tCurrent input: [ 1.  1.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.21901228 -0.2198023  -0.49643634 -0.55362209  0.60120196]\n",
      "\tCurrent input: [0. 0. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.24052017 -0.21109111 -0.50587571 -0.55075852  0.58810206]\n",
      "\tCurrent input: [-1.  0. -1. -1.  1.]\n",
      "New Weights:\n",
      "[-0.25035803 -0.20770749 -0.51128462 -0.54928407  0.5809005 ]\n",
      "\tCurrent input: [0. 1. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.23373727 -0.19733332 -0.51876412 -0.55263775  0.58082139]\n",
      "\tCurrent input: [ 0.  0.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.1808728  -0.17973867 -0.53306847 -0.56088007  0.58401997]\n",
      "\tCurrent input: [ 1.  1.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.18935999 -0.14259689 -0.54090143 -0.56345106  0.58221288]\n",
      "\tCurrent input: [0. 0. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.21708644 -0.14911175 -0.54133649 -0.55906209  0.57381021]\n",
      "\tCurrent input: [-1.  0. -1. -1.  1.]\n",
      "New Weights:\n",
      "[-0.22902091 -0.15292136 -0.54196335 -0.55697885  0.56947209]\n",
      "\tCurrent input: [0. 1. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.21273249 -0.14582636 -0.54616147 -0.55952108  0.57063658]\n",
      "\tCurrent input: [ 0.  0.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.16253551 -0.13603995 -0.55488047 -0.56581083  0.57490514]\n",
      "\tCurrent input: [ 1.  1.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.17339615 -0.10423059 -0.55831322 -0.56715097  0.57450418]\n",
      "\tCurrent input: [0. 0. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.20444466 -0.11826294 -0.55553067 -0.56248987  0.56828008]\n",
      "\tCurrent input: [-1.  0. -1. -1.  1.]\n",
      "New Weights:\n",
      "[-0.21754433 -0.12562748 -0.55437372 -0.56027265  0.5651807 ]\n",
      "\tCurrent input: [0. 1. 0. 0. 0.]\n",
      "New Weights:\n",
      "[-0.20155053 -0.12024111 -0.55721163 -0.56245575  0.56681898]\n",
      "\tCurrent input: [ 0.  0.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.15290102 -0.1144707  -0.56362293 -0.56790732  0.57147203]\n",
      "\tCurrent input: [ 1.  1.  1.  1. -1.]\n",
      "New Weights:\n",
      "[-0.16506268 -0.0853929  -0.56526324 -0.56872338  0.57160229]\n",
      "[-0.16506268 -0.0853929  -0.56526324 -0.56872338  0.57160229]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# do PCA and show the data in 1D\u001b[39;00m\n\u001b[1;32m     49\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m pca \u001b[38;5;241m=\u001b[39m \u001b[43mPCA\u001b[49m(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# fit the model parameters\u001b[39;00m\n\u001b[1;32m     52\u001b[0m pca\u001b[38;5;241m.\u001b[39mfit(data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implement Oja's rule here extending your previous implementation\n",
    "# of the simple Hebb rule. Note that this question on Oja's rule\n",
    "# uses a different dataset. So, don't forget to change the dataset\n",
    "# after you have copied the code to this block.\n",
    "\n",
    "\n",
    "\n",
    "# load data from the text file\n",
    "data = np.loadtxt('data_HL_simple.txt', usecols=range(5))\n",
    "print(data[1:5,:])\n",
    "\n",
    "# we need a vector of 5 parameters\n",
    "w = np.random.normal(0, 0.5, 5)\n",
    "\n",
    "print(\"initial weights\")\n",
    "print(w)\n",
    "\n",
    "# learning rate; don't change the learning rate epsilon when you are working on questions 2 and 3, unless you really know what you are doing. You may want to make the learning rate dependent on the epoch number to converge to a stable solution.\n",
    "epsilon = 0.01\n",
    "\n",
    "num_iterations = 6\n",
    "\n",
    "update = 0\n",
    "# we can do a fixed number of iterations because the simple Hebb rule won't converge\n",
    "for i in range(num_iterations):\n",
    "    for x in data:\n",
    "\n",
    "        # TODO: add your code here; note that \"epsilon\"\n",
    "        # is defined above and it should be used here;\n",
    "        # don't change its default value in this question.\n",
    "\n",
    "        y = np.dot(x,w)\n",
    "        delta_w = epsilon*(x*y-y*y*w)\n",
    "\n",
    "        w = w+delta_w\n",
    "        \n",
    "        update = update + 1\n",
    "        if update % 10 == 0:\n",
    "            # print every 10th update\n",
    "            print(\"\\tCurrent input: \" + str(x))\n",
    "            # print the weights for every x to see how they are evolving\n",
    "            print(\"New Weights:\\n\" + str(w))\n",
    "\n",
    "# the final weights\n",
    "print(w)\n",
    "\n",
    "\n",
    "# do PCA and show the data in 1D\n",
    "plt.figure(2)\n",
    "pca = PCA(n_components=1)\n",
    "# fit the model parameters\n",
    "pca.fit(data)\n",
    "print(\"all principal components:\")\n",
    "print(pca.components_)\n",
    "# project the data to the new dimension\n",
    "data_pca = pca.transform(data)\n",
    "# a vector of zeros for the scatter plot\n",
    "yaxis = np.zeros(data_pca.shape[0])\n",
    "plt.scatter(data_pca[:,0], yaxis)\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b3a96-e72e-4a04-a862-535dddc6083e",
   "metadata": {},
   "source": [
    "**(3-5):** Run your implementation of Oja's rule on `data_two_groups.txt`. Use the vector of parameters $w$ that you will obtain using Oja's rule to project the original five-dimensional data into 1 dimension defined by $w$. The image that you will obtain should be similar to the one produced by PCA in `sklearn` above. The images may be identical if you have run Oja's rule until convergence, but this is not guaranteed. This step will allow you to see very clearly how a low dimensional representation (and hence data compression) can be achieved using Hebbian learning/Oja's rule or PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20270600-ac91-4124-800f-a6a0669bdca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Oja's rule here extending your previous implementation\n",
    "# of the simple Hebb rule. Note that this question on Oja's rule\n",
    "# uses a different dataset. So, don't forget to change the dataset\n",
    "# after you have copied the code to this block.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# load data from the text file\n",
    "data = np.loadtxt('data_two_groups.txt', usecols=range(5))\n",
    "print(data[1:5,:])\n",
    "\n",
    "# we need a vector of 5 parameters\n",
    "w = np.random.normal(0, 0.5, 5)\n",
    "\n",
    "print(\"initial weights\")\n",
    "print(w)\n",
    "\n",
    "# learning rate; don't change the learning rate epsilon when you are working on questions 2 and 3, unless you really know what you are doing. You may want to make the learning rate dependent on the epoch number to converge to a stable solution.\n",
    "epsilon = 0.01\n",
    "\n",
    "num_iterations = 6\n",
    "\n",
    "update = 0\n",
    "# we can do a fixed number of iterations because the simple Hebb rule won't converge\n",
    "for i in range(num_iterations):\n",
    "    for x in data:\n",
    "\n",
    "        # TODO: add your code here; note that \"epsilon\"\n",
    "        # is defined above and it should be used here;\n",
    "        # don't change its default value in this question.\n",
    "\n",
    "        y = np.dot(x,w)\n",
    "        delta_w = epsilon*(x*y-y*y*w)\n",
    "\n",
    "        w = w+delta_w\n",
    "        \n",
    "        update = update + 1\n",
    "        if update % 10 == 0:\n",
    "            # print every 10th update\n",
    "            print(\"\\tCurrent input: \" + str(x))\n",
    "            # print the weights for every x to see how they are evolving\n",
    "            print(\"New Weights:\\n\" + str(w))\n",
    "\n",
    "# the final weights\n",
    "print(w)\n",
    "\n",
    "\n",
    "# do PCA and show the data in 1D\n",
    "plt.figure(2)\n",
    "pca = PCA(n_components=1)\n",
    "# fit the model parameters\n",
    "pca.fit(data)\n",
    "print(\"all principal components:\")\n",
    "print(pca.components_)\n",
    "# project the data to the new dimension\n",
    "data_pca = pca.transform(data)\n",
    "# a vector of zeros for the scatter plot\n",
    "yaxis = np.zeros(data_pca.shape[0])\n",
    "plt.scatter(data_pca[:,0], yaxis)\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d8b3e-b1ce-4f7a-93cb-bf487d24d0a5",
   "metadata": {},
   "source": [
    "**(1-2):** In the above code, identify Python variables that correspond to the symbol $y_j$ in our lecture notes. What is the range of $j$ in this example? How does the number of values of $y_j$ relate to the number of data examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34604cd4-f42b-4925-aa30-47c685e2e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Oja's rule here extending your previous implementation\n",
    "# of the simple Hebb rule. Note that this question on Oja's rule\n",
    "# uses a different dataset. So, don't forget to change the dataset\n",
    "# after you have copied the code to this block.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# load data from the text file\n",
    "data = np.loadtxt('data_two_groups.txt', usecols=range(5))\n",
    "print(data[1:5,:])\n",
    "\n",
    "# we need a vector of 5 parameters\n",
    "w = np.random.normal(0, 0.5, 5)\n",
    "\n",
    "print(\"initial weights\")\n",
    "print(w)\n",
    "\n",
    "# learning rate; don't change the learning rate epsilon when you are working on questions 2 and 3, unless you really know what you are doing. You may want to make the learning rate dependent on the epoch number to converge to a stable solution.\n",
    "epsilon = 0.01\n",
    "\n",
    "num_iterations = 6\n",
    "\n",
    "update = 0\n",
    "# we can do a fixed number of iterations because the simple Hebb rule won't converge\n",
    "for i in range(num_iterations):\n",
    "    for x in data:\n",
    "\n",
    "        # TODO: add your code here; note that \"epsilon\"\n",
    "        # is defined above and it should be used here;\n",
    "        # don't change its default value in this question.\n",
    "\n",
    "        y = np.dot(x,w)\n",
    "        delta_w = epsilon*(x*y-y*y*w)\n",
    "\n",
    "        w = w+delta_w\n",
    "        \n",
    "        update = update + 1\n",
    "        if update % 10 == 0:\n",
    "            # print every 10th update\n",
    "            print(\"\\tCurrent input: \" + str(x))\n",
    "            # print the weights for every x to see how they are evolving\n",
    "            print(\"New Weights:\\n\" + str(w))\n",
    "\n",
    "# the final weights\n",
    "print(w)\n",
    "\n",
    "\n",
    "# do PCA and show the data in 1D\n",
    "plt.figure(2)\n",
    "pca = PCA(n_components=1)\n",
    "# fit the model parameters\n",
    "pca.fit(data)\n",
    "print(\"all principal components:\")\n",
    "print(pca.components_)\n",
    "# project the data to the new dimension\n",
    "data_pca = pca.transform(data)\n",
    "# a vector of zeros for the scatter plot\n",
    "yaxis = np.zeros(data_pca.shape[0])\n",
    "plt.scatter(data_pca[:,0], yaxis)\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965240f6-ec26-4ef7-8163-7c4fada113de",
   "metadata": {},
   "source": [
    "**(3-1)** Compute `data_rec`, which is the reconstruction of the original data from its compressed 1D representation that is stored in `data_pca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9424dc0-87af-4d79-bd1b-7ceb620c4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Oja's rule here extending your previous implementation\n",
    "# of the simple Hebb rule. Note that this question on Oja's rule\n",
    "# uses a different dataset. So, don't forget to change the dataset\n",
    "# after you have copied the code to this block.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# load data from the text file\n",
    "data = np.loadtxt('data_two_groups.txt', usecols=range(5))\n",
    "print(data[1:5,:])\n",
    "\n",
    "# we need a vector of 5 parameters\n",
    "w = np.random.normal(0, 0.5, 5)\n",
    "\n",
    "print(\"initial weights\")\n",
    "print(w)\n",
    "\n",
    "# learning rate; don't change the learning rate epsilon when you are working on questions 2 and 3, unless you really know what you are doing. You may want to make the learning rate dependent on the epoch number to converge to a stable solution.\n",
    "epsilon = 0.01\n",
    "\n",
    "num_iterations = 6\n",
    "\n",
    "update = 0\n",
    "# we can do a fixed number of iterations because the simple Hebb rule won't converge\n",
    "for i in range(num_iterations):\n",
    "    for x in data:\n",
    "\n",
    "        # TODO: add your code here; note that \"epsilon\"\n",
    "        # is defined above and it should be used here;\n",
    "        # don't change its default value in this question.\n",
    "\n",
    "        y = np.dot(x,w)\n",
    "        delta_w = epsilon*(x*y-y*y*w)\n",
    "\n",
    "        w = w+delta_w\n",
    "        \n",
    "        update = update + 1\n",
    "        if update % 10 == 0:\n",
    "            # print every 10th update\n",
    "            print(\"\\tCurrent input: \" + str(x))\n",
    "            # print the weights for every x to see how they are evolving\n",
    "            print(\"New Weights:\\n\" + str(w))\n",
    "\n",
    "# the final weights\n",
    "print(w)\n",
    "\n",
    "\n",
    "# do PCA and show the data in 1D\n",
    "plt.figure(2)\n",
    "pca = PCA(n_components=1)\n",
    "# fit the model parameters\n",
    "pca.fit(data)\n",
    "print(\"all principal components:\")\n",
    "print(pca.components_)\n",
    "# project the data to the new dimension\n",
    "data_pca = pca.transform(data)\n",
    "# a vector of zeros for the scatter plot\n",
    "yaxis = np.zeros(data_pca.shape[0])\n",
    "plt.scatter(data_pca[:,0], yaxis)\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392df7cf-9c90-4601-913d-ba5b9fa29cdb",
   "metadata": {},
   "source": [
    "This question will require the students to engage with the scientific literature. The challenge is to compare the Delta Rule with the Perceptron learning algorithm. The comparison should highlight two differences of both methods. The differences can be related to both how these algorithms work and the properties of the learned models. A technical explanation is required, that is, every difference must be explained using technical terms. Cite the sources that will be used to answer this question. Use books or peer-reviewed scientific papers published in journals or conference-proceedings (blogs and websites are not in this category)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1619d335-1497-4ad9-917e-3148ac4d4adf",
   "metadata": {},
   "source": [
    "The Delta Rule and the Perceptron learning algorithm are both algorithms used for training artificial neural networks. Here are two key differences between these two methods:\r\n",
    "\r\n",
    "1. **Learning Rule:**\r\n",
    "   - **Delta Rule (Widrow-Hoff Rule):** The Delta Rule is a generalization of the Perceptron learning algorithm. It is used for updating the weights of connections in a neural network during supervised learning. Unlike the Perceptron learning algorithm, which only updates the weights based on the error in the output, the Delta Rule takes into account the continuous nature of the output. It uses the gradient descent approach to minimize the error by adjusting the weights in the direction of the steepest decrease in the error function. The Delta Rule considers the actual output of the neuron and aims to reduce the difference between the predicted and target values.\r\n",
    "  \r\n",
    "   - **Perceptron Learning Algorithm:** The Perceptron learning algorithm, on the other hand, is a binary classifier used for supervised learning of binary linear classifiers. It updates the weights based on the error in the predicted output compared to the target output. If the predicted output is correct, no adjustment is made. However, if there is a misclassification, the weights are updated to move the decision boundary in the direction that corrects the error. The Perceptron learning algorithm is specifically designed for binary classification tasks and is not well-suited for problems where the output is continuous.\r\n",
    "\r\n",
    "2. **Model Representation:**\r\n",
    "   - **Delta Rule:** The Delta Rule is commonly used for training multi-layer perceptrons, which are neural networks with one or more hidden layers. It allows for the learning of complex relationships between inputs and outputs by adjusting the weights in each layer. The Delta Rule is well-suited for problems where the output is a continuous function of the inputs, making it suitable for regression tasks.\r\n",
    "\r\n",
    "   - **Perceptron Learning Algorithm:** The Perceptron learning algorithm is primarily used for training single-layer perceptrons, where there is only one layer of weights connecting the input to the output. The decision boundary generated by the Perceptron is a hyperplane, and it is particularly effective for linearly separable binary classification problems. The Perceptron learning algorithm is limited in its ability to learn non-linear relationships between inputs and outputs, as it can only produce linear decision boundaries.\r\n",
    "\r\n",
    "In summary, the Delta Rule and the Perceptron learning algorithm differ in their learning rules, with the Delta Rule incorporating gradient descent for continuous output adjustment, and the Perceptron algorithm using a simple update rule for binary classification. Additionally, the Delta Rule is more versatile, being applicable to multi-layer perceptrons and regression tasks, while the Perceptron is limited to single-layer perceptrons and binary classification with linear decision boundaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
